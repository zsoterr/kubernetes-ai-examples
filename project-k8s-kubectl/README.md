# project-k8s-kubectl
Increase kubectl's  efficiently with AI capabilities

## What is it and why?
In this project, I will show you how to use capabilites of AI with kubectl in linux CLI, through a real-world examples. 
The goal is to:
-  provide you with practical examples if you are interested in Kubernetes cluster management from CLI with AI integration,
-  give you with clear instructions on how to set up the environment and how you can use it to further develop it in your own environment


## Key features
- Technologies used: `Kubernetes v1.3x`, `Python 3.xx`,
- Architecture: `<NA> …`
- Important files & folders:
  - `main.py` – entry point from Python  
  - `index.html` – GUI / web interface 
  - `README.md` – this file  
- License: **MIT License** – see `LICENSE` file in the parent folder

## Usage
1. **Pre-requisites**::
- **Ubuntu Linux**:
  ```bash
    apt install python3.12-venv
    python3 -m venv venv
    source venv/bin/activate
    pip install fastapi uvicorn[standard] jinja2
  ```  
- **Python**:
  You can use the requirements.txt in the "backend" directory if you want:
  ```bash
    pip install -r requirements.txt
  ```
- **Kubectl**:
  You already **configured** the *kubeconfig* file and **kubectl** "**works**": that means you can interact with your Kubernetes cluster with kubectl. Validate it.
- **OPENAI**:
  You already have an OPENAI API KEY (if you don't have one yet, create one).
3. **Configuation**:
- **main.py**:
  ```bash
    mkdir -p ~/kubectl-ai-web/templates && cd ~/kubectl-ai-web
  ```   
  **Create** the file with the right content (vi *~/kubectl-ai-web/main.py* ) **or** **copy** *main.py* file to this path: *~/kubectl-ai-web/main.py*
- **index.html**: \
  **Create** the file with the right content (vi *~/kubectl-ai-web/templates/index.html* ) **or** **copy** *index.html* file to this path: *~/kubectl-ai-web/templates/index.html*
5. **Start it**:
- Running FASTAPI **locally** on loopback interface of local host: \
  **Note**: change the *$YOUR-OPENAI-API-KEY* before you run this commands:
  ```bash
    cd ~/kubectl-ai-web
    source venv/bin/activate
    export OPENAI_API_KEY=$YOUR-OPENAI-API-KEY
    uvicorn main:app --host 127.0.0.1 --port 8000 &
  ``` 
6. **WebInterface**:
- Option one: \
  You **don't have graphical interface** on host where the FASTAPI is running and you want to use it "remotely":
  ```bash
    ssh -L 8000:localhost:8000 user@jump-host
   ```
  **Replace** the *user@jump-host* for yours. \
  and **open a browser** on your machine and **open this URL**: *http://localhost:8000* . \
  You will see the kubectl **web UI**, enter the prompt, "Run" and kubectl will run in the background on the jump host.
- Option two: \
  You **have graphical interface**  where the FASTAPI is running and you want to use on the same host. \
  Just **open a browser** and **open this URL**: *http://localhost:8000* . \
  You will see the kubectl **web UI**, enter the prompt, "Run" and kubectl will run in the background on the jump host.

  **Allow cluster changes**: \
  By default, it is **not possible to modify or change** the cluster status. This means, for example, that you cannot start or delete a pod.
If you want to perform such activities, you **must enable** this checkbox: "**Allow cluster changes**"

8. **Exit**: \
Close the browser and **identify** which **job** belongs to the FASTPI process:
  ```bash
    jobs
   ```
  let us **assume** that this is the output: *[1]+  Running uvicorn main:app --host 127.0.0.1 --port 8000 &* \
  **cancel** the job and use the job number (which is **1** in this case)
  ```bash
    kill -9 `jobs -p 1`
   ```

## Note
- This is the first version (v1), the goal was to have a working "demo" version: very basic, but I think it demonstrates well what it can be used for and how.
- It runs under Linux (I have validated it on Ubuntu 24.04 LTS.), on the local machine where the kubectl-ai program is installed. (not remote but you can use it remotely with ssh tunelling).
- It has a search function: you can search the outputs, it "remembers" the last 20 commands and outputs, you can copy the command generated by the AI and use (refine) it later if needed.
- single-user environment, no permission management, no database 

It's really **just a PoC**, using an openai API with a test kubernetes cluster.

## Planned changes in the near future:
- add other providers as further  "alternative" options, currently we use this definition in the code: **kubectl-ai --llm-provider=openai --model=gpt-4.1** that means the provider and the parameters are "hard-coded"
- context-switcher (kubeconfig)
- "Generate only" vs. "Generate + Run" mode (separate button)
- saved question-answer history (simple file or SQLite)
- multi-user ennvironment,
- "real" web server, enhanced UI
- scalability,
- security and performance-related sizing and configuration
- validation of RPM-based Linux distribution
- Run as pod in the Kubernetes cluster (K8s deployment, auth, RBAC)
